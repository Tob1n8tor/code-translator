{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model \n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create a dataframe for the V3 and V4 datasets\n",
    "def create_dataframe_for_V3_or_V4_data(model_path):\n",
    "    df = pd.read_csv(model_path)\n",
    "    df.rename(columns={'py': 'python', 'cpp':'c++'}, inplace=True)\n",
    "\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "    list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            for lan in df.columns:\n",
    "                if col != lan:\n",
    "                    new_row = {\n",
    "                        'input_language': col,\n",
    "                        'input_code': row[col],\n",
    "                        'target_language': lan,\n",
    "                        'target_code': row[lan]\n",
    "                    }\n",
    "                    list.append(new_row)\n",
    "\n",
    "    df = pd.DataFrame(list)\n",
    "\n",
    "    # Create a new column 'target_column' combining 'input_language' and 'target_language'\n",
    "    df['target_column'] = df['input_language'] + '_' + df['target_language']  \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create a dataframe for the V1 and V2 datasets\n",
    "def create_dataframe_for_V2_data(model_path):\n",
    "    df = pd.read_csv(model_path)\n",
    "    \n",
    "    # Create a new column 'target_column' combining 'input_language' and 'target_language'\n",
    "    df['target_column'] = df['input_language'] + '_' + df['target_language']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data CSV file into a pandas DataFrame\n",
    "model_path = os.path.join(\"datasets\", \"V3.csv\")\n",
    "\n",
    "df = create_dataframe_for_V2_data(model_path)\n",
    "#df = create_dataframe_for_V3_or_V4_data(model_path)\n",
    "\n",
    "test_size = 150  # Set to 150 as benchmarking would take too long with more test data  \n",
    "\n",
    "# Perform the stratified split for the test set (we're specifying the test set size in number of samples).\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=test_size, stratify=df['target_column'], random_state=42\n",
    ")\n",
    "\n",
    "# Now, split the train+val set into training (remaining data) and validation (5% of the remaining data).\n",
    "# Since we're working with a fixed test set size, the remaining data will be used for training and validation.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.05, stratify=train_val_df['target_column'], random_state=42\n",
    ")\n",
    "\n",
    "# Convert the DataFrames back into Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# display number of rows for train\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Check the splits\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Optional: Display the class distribution of the datasets to ensure balance\n",
    "print(f\"Train class distribution:\\n{train_df['target_column'].value_counts(normalize=True)}\")\n",
    "print(f\"Validation class distribution:\\n{val_df['target_column'].value_counts(normalize=True)}\")\n",
    "print(f\"Test class distribution:\\n{test_df['target_column'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./own_results\"\n",
    "\n",
    "# The fine-tuned model will be saved to this path\n",
    "saved_model_path  = os.path.join(\"training_codes\", \"<directory_name_for_saved_model>\")\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 10\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA and Quantization setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is different for each model. For example, for BERT, we need to use the `BertConfig` class and for GPT-2, we need to use the `GPT2Config` class.\n",
    "We decided to only display the setup for the codeT5 models, as the top three best performing fine-tuned models are codeT5 models.\n",
    "\n",
    "The setup code for the other models is quite similar. But suitable tokenizer and model classes should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config = bnb_config,\n",
    "    device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank (typically, smaller ranks like 4 or 8 work well)\n",
    "    lora_alpha=16,          # Scaling factor (adjust this for task-specific performance)\n",
    "    lora_dropout=0.1,       # Dropout to regularize LoRA parameters\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method, depends on model and used datsaset so please change depending on it\n",
    "def preprocess_function1(examples):\n",
    "    prefixes = [\n",
    "        f\"translate {lang1} to {lang2}:\"\n",
    "        for lang1, lang2 in zip(examples['input_language'], examples['target_language'])\n",
    "    ]\n",
    "\n",
    "    inputs = [\n",
    "        prefix + src_code\n",
    "        for prefix, src_code in zip(prefixes, examples['input_code'])\n",
    "    ]\n",
    "    targets = examples['target_code']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    # Add labels to inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_train1 = train_dataset.map(preprocess_function1, batched=True)\n",
    "tokenized_datasets_test1 = test_dataset.map(preprocess_function1, batched=True)\n",
    "tokenized_datasets_val1 = val_dataset.map(preprocess_function1, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/early_stopping_lora_4bit_quant_codeT5-base-own-dataV4_10_epochs_batchsize_8\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=2000,\n",
    "    learning_rate=2e-4,  # play around with it\n",
    "    per_device_train_batch_size=8,  # play around with it\n",
    "    per_device_eval_batch_size=8, # play around with it\n",
    "    gradient_accumulation_steps = 2,\n",
    "    optim = \"adamw_torch\",\n",
    "    weight_decay=0.001,  \n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = -1,\n",
    "    warmup_ratio = 0.03,\n",
    "    group_by_length = True,                   # speeds up the training\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(  \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_train1,\n",
    "    eval_dataset=tokenized_datasets_val1,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual check of the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = saved_model_path\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = tokenizer.decode(tokenized_datasets_test1[1][\"input_ids\"], skip_special_tokens=True) #\"translate java to python: public int div(int a, int b) {\\n    return a / b;\\n}\"\n",
    "inputs = tokenizer(promt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate translation\n",
    "outputs = model.generate(inputs[\"input_ids\"],   max_length=1024)\n",
    "translated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(translated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_datasets_test1[1][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids=torch.tensor([tokenized_datasets_test1[1][\"input_ids\"]]), max_length=1024)\n",
    "translated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Translated code:\", translated_code)\n",
    "print(\"Target code:\", tokenizer.decode(tokenized_datasets_test1[1][\"labels\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Print a few log entries\n",
    "for log in log_history:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract logged history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation loss from log history\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "steps_train = []\n",
    "steps_eval = []\n",
    "\n",
    "for entry in log_history:\n",
    "    if 'loss' in entry:  # Training loss\n",
    "        train_loss.append(entry['loss'])\n",
    "        steps_train.append(entry['step'])\n",
    "    if 'eval_loss' in entry:  # Validation loss\n",
    "        eval_loss.append(entry['eval_loss'])\n",
    "        steps_eval.append(entry['step'])\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps_train, train_loss, label=\"Training Loss\", marker='o')\n",
    "plt.plot(steps_eval, eval_loss, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
