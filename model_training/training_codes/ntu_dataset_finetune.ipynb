{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_translation_dataset = datasets.load_dataset(\"NTU-NLP-sg/xCodeEval\", 'code_translation')\n",
    "\n",
    "print(code_translation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_translation_dataset.save_to_disk('./code_translation_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_translation_dataset['train'][4380574]['lang_cluster'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Pairing\n",
    "\n",
    "We must create code-code pairs in order to train our model. For that we use the method described in https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval/discussions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training environment\n",
    "from datasets import load_from_disk\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NTU dataset was delivert together with all the models as it is 20 GB in size\n",
    "code_translation_dataset = load_from_disk('directory/to/ntu/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a pandas dataframe\n",
    "dataframe_code_translation_train = code_translation_dataset['train'].shuffle(seed=42).select(range(50000)).to_pandas()\n",
    "dataframe_code_translation_test = code_translation_dataset['titan'].shuffle(seed=42).select(range(7000)).to_pandas()\n",
    "dataframe_code_translation_val = code_translation_dataset['compact'].shuffle(seed=42).select(range(5000)).to_pandas()\n",
    "dataframe_code_translation_val_small = code_translation_dataset['compact_small'].to_pandas()\n",
    "\n",
    "# group by src_uid\n",
    "grouped_train = dataframe_code_translation_train.groupby('src_uid')\n",
    "grouped_test = dataframe_code_translation_test.groupby('src_uid')\n",
    "grouped_val = dataframe_code_translation_val.groupby('src_uid')\n",
    "grouped_val_small = dataframe_code_translation_val_small.groupby('src_uid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique lang_clusters per src_uid\n",
    "lang_cluster_counts = dataframe_code_translation_train.groupby('src_uid')['lang_cluster'].nunique()\n",
    "\n",
    "# Display the distribution of unique lang_clusters per group\n",
    "print(lang_cluster_counts.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped.get_group(code_translation_dataset['compact']['src_uid'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(grouped):\n",
    "    result = []\n",
    "\n",
    "    languages = grouped['lang_cluster'].unique()\n",
    "\n",
    "    for (lang_cluster_1, lang_cluster_2) in itertools.permutations(languages, 2):\n",
    "        \n",
    "        lang_cluster_1_rows = grouped[grouped['lang_cluster'] == lang_cluster_1]\n",
    "        lang_cluster_2_rows = grouped[grouped['lang_cluster'] == lang_cluster_2]\n",
    "\n",
    "         # Pair each row from lang_cluster_1 with each row from lang_cluster_2\n",
    "        for _, row1 in lang_cluster_1_rows.iterrows():\n",
    "            for _, row2 in lang_cluster_2_rows.iterrows():\n",
    "                result.append({\n",
    "                    'src_uid': row1['src_uid'],  # src_uid is the same for both rows in the pair\n",
    "                    'input_language': lang_cluster_1,\n",
    "                    'input_code': row1['source_code'],\n",
    "                    'target_language': lang_cluster_2,\n",
    "                    'target_code': row2['source_code']\n",
    "                })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "for name, group in grouped:\n",
    "    print(group.columns)\n",
    "    break\n",
    "\"\"\"    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each group\n",
    "grouped_pairs_train = grouped_train.apply(create_pairs)\n",
    "grouped_pairs_test = grouped_test.apply(create_pairs)\n",
    "grouped_pairs_val = grouped_val.apply(create_pairs)\n",
    "grouped_pairs_val_small = grouped_val_small.apply(create_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the lengths of all lists in the Series\n",
    "num_rows = sum(len(pairs) for pairs in grouped_pairs_train)\n",
    "\n",
    "print(f\"Total number of pairs: {num_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df_train = pd.DataFrame([item for sublist in grouped_pairs_train for item in sublist])\n",
    "flattened_df_test = pd.DataFrame([item for sublist in grouped_pairs_test for item in sublist])\n",
    "flattened_df_val = pd.DataFrame([item for sublist in grouped_pairs_val for item in sublist])\n",
    "flattened_df_val_small = pd.DataFrame([item for sublist in grouped_pairs_val_small for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_language', 'input_code', 'target_language', 'target_code']\n",
    "\n",
    "# Remove the 'src_uid' column from each DataFrame if it exists\n",
    "flattened_df_train = flattened_df_train.drop(columns=['src_uid'], errors='ignore')\n",
    "flattened_df_test = flattened_df_test.drop(columns=['src_uid'], errors='ignore')\n",
    "flattened_df_val = flattened_df_val.drop(columns=['src_uid'], errors='ignore')\n",
    "flattened_df_val_small = flattened_df_val_small.drop(columns=['src_uid'], errors='ignore')\n",
    "\n",
    "allowed_pairs = [\n",
    "    ('Java', 'Kotlin'),\n",
    "    ('Python', 'Kotlin'),\n",
    "    ('C', 'Kotlin'),\n",
    "    ('C++', 'Kotlin'),\n",
    "    ('C#', 'Kotlin')\n",
    "]\n",
    "\n",
    "allowed_pairs_set = set(allowed_pairs)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat([flattened_df_train, flattened_df_test, flattened_df_val, flattened_df_val_small], ignore_index=True)\n",
    "\n",
    "combined_df\n",
    "\n",
    "# Filter the rows where (input_language, target_language) is in the allowed pairs\n",
    "filtered_df = combined_df[combined_df.apply(lambda row: (row['input_language'], row['target_language']) in allowed_pairs_set, axis=1)]\n",
    "\n",
    "filtered_df.to_csv('filtered_combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df_train.groupby('src_uid').get_group(flattened_df_train['src_uid'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"Salesforce/codet5-small\"\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./NTU_results\"\n",
    "\n",
    "saved_model_path = \"./code_translation_model_2_epochs_50k_train\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# TODO: try quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function for correct input format\n",
    "def preprocess_function(examples):\n",
    "    prefixes = [\n",
    "        f\"translate {lang1} to {lang2}:\"\n",
    "        for lang1, lang2 in zip(examples['lang_cluster_1'], examples['lang_cluster_2'])\n",
    "    ]\n",
    "    inputs = [\n",
    "        prefix + src_code\n",
    "        for prefix, src_code in zip(prefixes, examples['src_code_1'])\n",
    "    ]\n",
    "    targets = examples['src_code_2']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    # Add labels to inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(flattened_df_train)\n",
    "ds_test = Dataset.from_pandas(flattened_df_test)\n",
    "ds_val = Dataset.from_pandas(flattened_df_val)\n",
    "ds_val_small = Dataset.from_pandas(flattened_df_val_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_train = ds_train.map(preprocess_function, batched=True)\n",
    "tokenized_datasets_test = ds_test.map(preprocess_function, batched=True)\n",
    "tokenized_datasets_val = ds_val.map(preprocess_function, batched=True)\n",
    "tokenized_datasets_val_small = ds_val_small.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_datasets_train[0]['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for eval during training\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,  # play around with it\n",
    "    per_device_train_batch_size=4,  # play around with it\n",
    "    per_device_eval_batch_size=4, # play around with it\n",
    "    gradient_accumulation_steps = 2,\n",
    "    optim = \"paged_adamw_32bit\", \n",
    "    weight_decay=0.001,  \n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = -1,\n",
    "    save_total_limit=3,\n",
    "    warmup_ratio = 0.03,\n",
    "    group_by_length = True,                   # speeds up the training\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=False,\n",
    "    report_to = \"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_val_small,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "# Train model\n",
    "trainer.train()\n",
    "model.save_pretrained(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First manuel check of the fine-tuned model output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(saved_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenized_datasets_test[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets_test[0][\"lang_cluster_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids=torch.tensor([tokenized_datasets_test[0][\"input_ids\"]]), max_length=1024)\n",
    "translated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Translated code:\", translated_code)\n",
    "print(\"Target code:\", tokenizer.decode(tokenized_datasets_test[0][\"labels\"], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
