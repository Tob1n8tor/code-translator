{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Excel File which contains the data being visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file to check its structure and contents\n",
    "file_path = model_path = os.path.join(\"excel_result_files\", \"overall_performance_results.xlsx\")\n",
    "data = pd.ExcelFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the first sheet to examine its content\n",
    "chart_data = data.parse('Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique models\n",
    "models = chart_data['model_name'].unique()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the evaluation metric on which the data is being visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the 'f1' score\n",
    "def extract_f1(score_str):\n",
    "    # Convert the string representation of the dictionary into an actual dictionary\n",
    "    score_dict = ast.literal_eval(score_str)\n",
    "    return score_dict['f1']\n",
    "\n",
    "# Apply the function to the 'bert_score' column\n",
    "# may be changed to any other metric column in the excel file\n",
    "chart_data['bert_f1_score'] = chart_data['bert_score'].apply(extract_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data['bert_f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots for the evaluation metric for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate bar plots for each model\n",
    "for model in models:\n",
    "    model_data = chart_data[chart_data['model_name'] == model]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(model_data['dataset_name'], model_data['bert_f1_score'], alpha=0.7, label='bert_score f1')\n",
    "\n",
    "    # Chart formatting\n",
    "    plt.title(f'Performance Metrics for {model}', fontsize=14)\n",
    "    plt.xlabel('Dataset Name', fontsize=12)\n",
    "    plt.ylabel('Scores', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performing fine-tuned model for each pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include hyperparameters in the cleaned dataset for best performance analysis\n",
    "best_performance = chart_data.loc[chart_data.groupby('model_name')['bert_f1_score'].idxmax()]\n",
    "best_performance = best_performance[['model_name', 'dataset_name', 'bert_f1_score', 'hyperparameters']].reset_index(drop=True)\n",
    "\n",
    "# Create a bar plot for the best-performing dataset by model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(best_performance['model_name'], best_performance['bert_f1_score'], alpha=0.7, color='blue')\n",
    "\n",
    "# Annotate each bar with its corresponding hyperparameters\n",
    "for i, row in best_performance.iterrows():\n",
    "    plt.text(i, row['bert_f1_score'] + 0.01, str(row['hyperparameters']), \n",
    "             ha='center', fontsize=8, rotation=90, wrap=True)\n",
    "\n",
    "# Chart formatting\n",
    "plt.title('Best-Performing Training Dataset for Each Model', fontsize=14)\n",
    "plt.xlabel('Model Name', fontsize=12)\n",
    "plt.ylabel('BERT f1 Score', fontsize=12)\n",
    "plt.xticks(ticks=range(len(best_performance)), labels=best_performance['model_name'], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show chart\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
