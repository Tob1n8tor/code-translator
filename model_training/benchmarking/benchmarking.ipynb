{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarking_metrics import benchmark_model\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM, M2M100ForConditionalGeneration, M2M100Tokenizer, AutoModelForSeq2SeqLM, EncoderDecoderModel, EncoderDecoderConfig,AutoConfig \n",
    "import numpy as np\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (may be any of the datasets in output_csvs)\n",
    "test_df_path = os.path.join(\"output_csvs\", \"java_python.csv\")\n",
    "test_df = pd.read_csv(test_df_path)\n",
    "\n",
    "# Store result of benchmarking in this excel file\n",
    "result_path = os.path.join(\"excel_result_files\", \"java_python_benchmarking.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The pre-trained model that was further fine-tuned \n",
    "model_name = \"<base model name>\"\n",
    "\n",
    "# The path to the saved model\n",
    "saved_model_path = \"<path to saved model>\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, EncoderDecoderModel, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base models first\n",
    "encoder = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "decoder = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Create the encoder-decoder model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"microsoft/codebert-base\",\n",
    "    \"roberta-base\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Load the LoRA configuration\n",
    "peft_config = PeftConfig.from_pretrained(\n",
    "    \"/home/tobias-konieczny/Schreibtisch/kabul/model_training/codebert-lora-early-stopping-finetuned\"\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "model = PeftModel.from_pretrained(model, \n",
    "    \"/home/tobias-konieczny/Schreibtisch/kabul/model_training/codebert-lora-early-stopping-finetuned\"\n",
    ")\n",
    "\n",
    "# Initialize tokenizers\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "\n",
    "# use this to load the alireyamsh/small100 model\n",
    "\"\"\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.src_lang = \"en\"\n",
    "tokenizer.tgt_lang = \"en\"\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\"\"\"\n",
    "\n",
    "# use this to load gpt2 model\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\"\"\"\n",
    "\n",
    "# use this to load codeT5 models which have been fine-tuned using the PEFT method\n",
    "\"\"\"\n",
    "peft_model_id = saved_model_path\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "\"\"\"\n",
    "\n",
    "# use this to load codeT5 models which have been fine-tuned without using the PEFT method\n",
    "# this can also be used to load fine-tuned codebert models\n",
    "\"\"\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method, depends on model and used datsaset so please change depending on it\n",
    "def preprocess_function1(examples):\n",
    "    prefixes = [\n",
    "        f\"translate {lang1} to {lang2}:\"\n",
    "        for lang1, lang2 in zip(examples['input_language'], examples['target_language'])\n",
    "    ]\n",
    "\n",
    "    inputs = [\n",
    "        prefix + src_code\n",
    "        for prefix, src_code in zip(prefixes, examples['input_code'])\n",
    "    ]\n",
    "    targets = examples['target_code']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = encoder_tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with encoder_tokenizer.as_target_tokenizer():\n",
    "        labels = encoder_tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    # Add labels to inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_test1 = test_dataset.map(preprocess_function1, batched=True)\n",
    "#tokenized_datasets_test2 = test_dataset.map(preprocess_function2, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test data with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this one for codebert base:\n",
    "# predict on test set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "preds = []\n",
    "for test in tqdm(tokenized_datasets_test1, desc=\"Generating Predictions\"):\n",
    "    input_ids = torch.tensor([test[\"input_ids\"]]).to(device)\n",
    "    output = model.generate(input_ids=input_ids, max_length=50, do_sample=False)\n",
    "    translated_code = decoder_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    preds.append(translated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this one for all the other models\n",
    "# predict on test set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "preds = []\n",
    "for test in tqdm(tokenized_datasets_test1, desc=\"Generating Predictions\"):\n",
    "    input_ids = torch.tensor([test[\"input_ids\"]]).to(device)\n",
    "    output = model.generate(input_ids=input_ids, max_length=1024)\n",
    "    translated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    preds.append(translated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of true lables for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save reference\n",
    "refs = []\n",
    "for test in tqdm(tokenized_datasets_test1, desc=\"Generating Predictions\"):\n",
    "    refs.append(decoder_tokenizer.decode(test[\"labels\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds[0])\n",
    "print(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using the test data and the benchmarking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"weight_decay\": \"todo\",\n",
    "    \"max_grad_norm\": \"todo\",\n",
    "    \"warmup_ratio\": 0.1\n",
    "}\n",
    "\n",
    "benchmarks = benchmark_model(model_name=model_name, dataset_name=\"datasetname\", training_size=\"# pairs\", hyperparameters=hyperparameters,\n",
    "                              preds=preds, refs=refs, result_path=result_path, save_results=True, lora=False, quantization=False, earlystopping=True)\n",
    "print(benchmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextGenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
